{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7aec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='rock-paper-scissors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e726e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'val']\n",
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "# Data directory\n",
    "data_dir = './datasets/Datasets/Datasets'\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "# Classes\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb730e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "train_tfms = tt.Compose([tt.CenterCrop(224),\n",
    "                         tt.Resize((32,32)), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "\n",
    "valid_tfms = tt.Compose([tt.CenterCrop(224),tt.Resize((32,32)),tt.ToTensor(), tt.Normalize(*stats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4e4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "train_dataset = ImageFolder(data_dir+'/train', train_tfms)\n",
    "valid_dataset = ImageFolder(data_dir+'/val', valid_tfms)\n",
    "test_dataset = ImageFolder(data_dir+'/test', valid_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88597fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_of_classes = len(classes)\n",
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15316eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data loaders\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08643155",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_images(dataloader):\n",
    "    for images, labels in dataloader:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(train_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dcc30f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "79728bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "740b583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Dataset on the device('Cuda' if available)\n",
    "train_dataloader = DeviceDataLoader(train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cea43ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa19c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class BasicResNet9(nn.Module):\n",
    "    def __init__(self, in_channels, num_of_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Linear(512, num_of_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf65a1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "basicResNetModel = BasicResNet9(3,3)\n",
    "basicResNetModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c83ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the ResNet9 model\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(512, 3)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                nn.Flatten(), \n",
    "#                                 nn.Dropout(0.2),\n",
    "                                nn.Linear(512, num_of_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c8ee1019",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet9(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=512, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "resNet9Model = ResNet9()\n",
    "resNet9Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1a293160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallResNet9, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "#         self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(256, num_of_classes)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4), \n",
    "                      nn.Flatten(), \n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, num_of_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "#         x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5d81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5\n",
    "number_of_epochs = 40\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a37e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909c1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation function\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3e93f9a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "1926f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Epoch 1 - Train loss: 0.9170 - Val loss: 1.2948 - Val accuracy: 0.3281\n",
      "Epoch 2 - Train loss: 0.7825 - Val loss: 1.1810 - Val accuracy: 0.5646\n",
      "Epoch 3 - Train loss: 0.7033 - Val loss: 1.1367 - Val accuracy: 0.6981\n",
      "Epoch 4 - Train loss: 0.6666 - Val loss: 1.0669 - Val accuracy: 0.7234\n",
      "Epoch 5 - Train loss: 0.6474 - Val loss: 1.0323 - Val accuracy: 0.7173\n",
      "Epoch 6 - Train loss: 0.6379 - Val loss: 0.9692 - Val accuracy: 0.8386\n",
      "Epoch 7 - Train loss: 0.6260 - Val loss: 0.9543 - Val accuracy: 0.8429\n",
      "Epoch 8 - Train loss: 0.6201 - Val loss: 0.9038 - Val accuracy: 0.8813\n",
      "Epoch 9 - Train loss: 0.6108 - Val loss: 0.8665 - Val accuracy: 0.8473\n",
      "Epoch 10 - Train loss: 0.6059 - Val loss: 0.8391 - Val accuracy: 0.8796\n",
      "Epoch 11 - Train loss: 0.6046 - Val loss: 0.8103 - Val accuracy: 0.8813\n",
      "Epoch 12 - Train loss: 0.5988 - Val loss: 0.7934 - Val accuracy: 0.8787\n",
      "Epoch 13 - Train loss: 0.5963 - Val loss: 0.7853 - Val accuracy: 0.8604\n",
      "Epoch 14 - Train loss: 0.5967 - Val loss: 0.7737 - Val accuracy: 0.9197\n",
      "Epoch 15 - Train loss: 0.5857 - Val loss: 0.7367 - Val accuracy: 0.9206\n",
      "Epoch 16 - Train loss: 0.5828 - Val loss: 0.7257 - Val accuracy: 0.9354\n",
      "Epoch 17 - Train loss: 0.5808 - Val loss: 0.7142 - Val accuracy: 0.9302\n",
      "Epoch 18 - Train loss: 0.5782 - Val loss: 0.7029 - Val accuracy: 0.9250\n",
      "Epoch 19 - Train loss: 0.5771 - Val loss: 0.7002 - Val accuracy: 0.9459\n",
      "Epoch 20 - Train loss: 0.5742 - Val loss: 0.6975 - Val accuracy: 0.9468\n",
      "Epoch 21 - Train loss: 0.5716 - Val loss: 0.6836 - Val accuracy: 0.9529\n",
      "Epoch 22 - Train loss: 0.5701 - Val loss: 0.6797 - Val accuracy: 0.9572\n",
      "Epoch 23 - Train loss: 0.5704 - Val loss: 0.6663 - Val accuracy: 0.9538\n",
      "Epoch 24 - Train loss: 0.5681 - Val loss: 0.6694 - Val accuracy: 0.9634\n",
      "Epoch 25 - Train loss: 0.5658 - Val loss: 0.6611 - Val accuracy: 0.9642\n",
      "Epoch 26 - Train loss: 0.5658 - Val loss: 0.6528 - Val accuracy: 0.9625\n",
      "Epoch 27 - Train loss: 0.5620 - Val loss: 0.6541 - Val accuracy: 0.9677\n",
      "Epoch 28 - Train loss: 0.5635 - Val loss: 0.6550 - Val accuracy: 0.9695\n",
      "Epoch 29 - Train loss: 0.5625 - Val loss: 0.6449 - Val accuracy: 0.9634\n",
      "Epoch 30 - Train loss: 0.5617 - Val loss: 0.6429 - Val accuracy: 0.9459\n",
      "Epoch 31 - Train loss: 0.5617 - Val loss: 0.6444 - Val accuracy: 0.9651\n",
      "Epoch 32 - Train loss: 0.5607 - Val loss: 0.6337 - Val accuracy: 0.9756\n",
      "Epoch 33 - Train loss: 0.5598 - Val loss: 0.6313 - Val accuracy: 0.9564\n",
      "Epoch 34 - Train loss: 0.5596 - Val loss: 0.6235 - Val accuracy: 0.9738\n",
      "Epoch 35 - Train loss: 0.5592 - Val loss: 0.6231 - Val accuracy: 0.9677\n",
      "Epoch 36 - Train loss: 0.5592 - Val loss: 0.6246 - Val accuracy: 0.9616\n",
      "Epoch 37 - Train loss: 0.5573 - Val loss: 0.6202 - Val accuracy: 0.9799\n",
      "Epoch 38 - Train loss: 0.5577 - Val loss: 0.6163 - Val accuracy: 0.9825\n",
      "Epoch 39 - Train loss: 0.5573 - Val loss: 0.6139 - Val accuracy: 0.9782\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [42]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;66;03m# Train and validate for this fold\u001b[39;00m\n\u001b[0;32m     31\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_epochs):\n\u001b[1;32m---> 32\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m#         val_loss = evaluate(model, val_loader)\u001b[39;00m\n\u001b[0;32m     34\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m train(model,val_loader,criterion,optimizer)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store training and validation losses and accuracies\n",
    "\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k}')\n",
    "    \n",
    "    # Create subset datasets and dataloaders for this fold\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    # Define model,\n",
    "    model = SmallResNet9()\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#     optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(number_of_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer)\n",
    "#         val_loss = evaluate(model, val_loader)\n",
    "        val_loss = train(model,val_loader,criterion,optimizer)\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Save the model for this fold\n",
    "#     torch.save(model.state_dict(), f'resnet9_fold{fold}.pt')\n",
    "    \n",
    "# Print overall validation accuracy\n",
    "print(f'Overall validation accuracy: {np.mean(val_accuracies):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "49285d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model, './saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "71cfac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the losses\n",
    "def plot_losses():\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4c1a0de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "ef22f4d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ds = ImageFolder(data_dir+'/test', valid_tfms)\n",
    "test_dl = DataLoader(test_ds, batch_size, num_workers=3, pin_memory=True)\n",
    "test_dl = DeviceDataLoader(test_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "c7c9d1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallResNet9(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "test_model = ResNet9()\n",
    "PATH = 'saved_model.pth'\n",
    "test_model = torch.load(\"saved_model.pth\", map_location = device)\n",
    "test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ec876665",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_model.eval()\n",
    "# correct = 0\n",
    "# total = 0\n",
    "# with torch.no_grad():\n",
    "#     for data in test_dl:\n",
    "#         images, labels = data\n",
    "#         images, labels = images.to(device), labels.to(device)\n",
    "#         outputs = test_model(images)\n",
    "#         _, predicted = torch.max(outputs.data, 1)\n",
    "#         total += labels.size(0)\n",
    "#         correct += (predicted == labels).sum().item()\n",
    "\n",
    "# print('Accuracy of the ResNet9 model on the %d test images: %d %%' % (total, 100 * correct / total))\n",
    "\n",
    "def evaluate_test_data(model, val_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = test_model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    f1score = f1_score(all_labels, all_preds, average='weighted')\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print('F1 Score:- ', f1score)\n",
    "    print('Accuracy:- ', accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "f6f59e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 Score:-  0.6895459007900271\n",
      "Accuracy:-  0.6864035087719298\n"
     ]
    }
   ],
   "source": [
    "evaluate_test_data(test_model,test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93449a1d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
