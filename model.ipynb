{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7aec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision.transforms as transform\n",
    "import torchmetrics\n",
    "import random\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as matplot\n",
    "\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset, random_split\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from torchmetrics import F1Score, Accuracy\n",
    "\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='rock-paper-scissors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e726e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['paper', 'rock', 'scissors']\n",
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "# Data directory\n",
    "data_dir = './dataset'\n",
    "print(os.listdir(data_dir))\n",
    "\n",
    "# Classes\n",
    "classes = os.listdir(data_dir)\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb730e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "train_tfms = transform.Compose([transform.CenterCrop(224),\n",
    "                         transform.Resize((32,32)), \n",
    "                         transform.RandomHorizontalFlip(), \n",
    "                         transform.ToTensor(), \n",
    "                         transform.Normalize(*stats,inplace=True)])\n",
    "\n",
    "valid_tfms = transform.Compose([transform.CenterCrop(224),transform.Resize((32,32)),transform.ToTensor(), transform.Normalize(*stats)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fa4e4ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Datasets\n",
    "# train_dataset = ImageFolder(data_dir+'/train', train_tfms)\n",
    "# valid_dataset = ImageFolder(data_dir+'/validation', valid_tfms)\n",
    "# test_dataset = ImageFolder(data_dir+'/test', valid_tfms)\n",
    "# dataset = ImageFolder(data_dir)\n",
    "rock_paper_scissors_data = ImageFolder(data_dir, train_tfms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26972669",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.85 * len(rock_paper_scissors_data))\n",
    "test_size = len(rock_paper_scissors_data) - train_size\n",
    "train_dataset, test_dataset = random_split(rock_paper_scissors_data, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "88597fd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_size = 64\n",
    "num_of_classes = len(classes)\n",
    "num_of_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15316eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loaders\n",
    "# train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "# val_dataloader = DataLoader(valid_dataset, batch_size=batch_size, num_workers=3, pin_memory=True)\n",
    "# test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5a656c6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08643155",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to show images\n",
    "def show_images(dataloader):\n",
    "    for images, labels in dataloader:\n",
    "        fig, ax = matplot.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ab190b",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_images(test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dcc30f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "79728bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "740b583b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Dataloader on the device('Cuda' if available)\n",
    "train_dataloader = DeviceDataLoader(train_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c83ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ResNet9 model\n",
    "class ResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4),\n",
    "                                nn.Flatten(), \n",
    "                                nn.Dropout(0.2),\n",
    "                                nn.Linear(512, num_of_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a293160",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SmallResNet9 model\n",
    "class SmallResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SmallResNet9, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4), \n",
    "                      nn.Flatten(), \n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, num_of_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.fc(x)\n",
    "        x = F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4f5d81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 10\n",
    "number_of_epochs = 50\n",
    "learning_rate = 0.001\n",
    "weight_decay = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a3304b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# F-1 Score\n",
    "def get_f1score(preds, labels):\n",
    "    f1_score = F1Score(task=\"multiclass\", num_classes=num_of_classes, average='weighted')\n",
    "    print(f1_score)\n",
    "    return f1_score(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "80ce2dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(preds, labels):\n",
    "    accuracy = Accuracy(task=\"multiclass\", num_classes=num_of_classes).to(device)\n",
    "    return accuracy(preds, labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4a37e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "edc96273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    return all_preds, all_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1926f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/10\n",
      "Epoch 1 - Train loss: 0.9305 - Val loss: 0.8021 - Val accuracy: 0.7972\n",
      "Epoch 2 - Train loss: 0.7402 - Val loss: 0.7267 - Val accuracy: 0.8491\n",
      "Epoch 3 - Train loss: 0.6996 - Val loss: 0.6919 - Val accuracy: 0.8774\n",
      "Epoch 4 - Train loss: 0.6763 - Val loss: 0.6909 - Val accuracy: 0.8145\n",
      "Epoch 5 - Train loss: 0.6599 - Val loss: 0.6590 - Val accuracy: 0.8601\n",
      "Epoch 6 - Train loss: 0.6491 - Val loss: 0.6537 - Val accuracy: 0.9088\n",
      "Epoch 7 - Train loss: 0.6399 - Val loss: 0.6524 - Val accuracy: 0.9041\n",
      "Epoch 8 - Train loss: 0.6354 - Val loss: 0.6426 - Val accuracy: 0.9277\n",
      "Epoch 9 - Train loss: 0.6309 - Val loss: 0.6302 - Val accuracy: 0.9434\n",
      "Epoch 10 - Train loss: 0.6190 - Val loss: 0.6227 - Val accuracy: 0.9308\n",
      "Epoch 11 - Train loss: 0.6185 - Val loss: 0.6123 - Val accuracy: 0.9465\n",
      "Epoch 12 - Train loss: 0.6074 - Val loss: 0.6014 - Val accuracy: 0.9591\n",
      "Epoch 13 - Train loss: 0.6065 - Val loss: 0.6152 - Val accuracy: 0.9481\n",
      "Epoch 14 - Train loss: 0.6031 - Val loss: 0.5976 - Val accuracy: 0.9544\n",
      "Epoch 15 - Train loss: 0.5980 - Val loss: 0.6007 - Val accuracy: 0.9560\n",
      "Epoch 16 - Train loss: 0.5986 - Val loss: 0.5919 - Val accuracy: 0.9701\n",
      "Epoch 17 - Train loss: 0.5918 - Val loss: 0.5872 - Val accuracy: 0.9764\n",
      "Epoch 18 - Train loss: 0.5879 - Val loss: 0.5900 - Val accuracy: 0.9701\n",
      "Epoch 19 - Train loss: 0.5874 - Val loss: 0.5946 - Val accuracy: 0.9277\n",
      "Epoch 20 - Train loss: 0.5853 - Val loss: 0.5799 - Val accuracy: 0.9811\n",
      "Epoch 21 - Train loss: 0.5829 - Val loss: 0.5766 - Val accuracy: 0.9796\n",
      "Epoch 22 - Train loss: 0.5793 - Val loss: 0.5772 - Val accuracy: 0.9827\n",
      "Epoch 23 - Train loss: 0.5791 - Val loss: 0.5804 - Val accuracy: 0.9748\n",
      "Epoch 24 - Train loss: 0.5773 - Val loss: 0.5705 - Val accuracy: 0.9906\n",
      "Epoch 25 - Train loss: 0.5749 - Val loss: 0.5764 - Val accuracy: 0.9764\n",
      "Epoch 26 - Train loss: 0.5734 - Val loss: 0.5687 - Val accuracy: 0.9858\n",
      "Epoch 27 - Train loss: 0.5724 - Val loss: 0.5727 - Val accuracy: 0.9890\n",
      "Epoch 28 - Train loss: 0.5713 - Val loss: 0.5717 - Val accuracy: 0.9843\n",
      "Epoch 29 - Train loss: 0.5701 - Val loss: 0.5740 - Val accuracy: 0.9874\n",
      "Epoch 30 - Train loss: 0.5674 - Val loss: 0.5620 - Val accuracy: 0.9890\n",
      "Epoch 31 - Train loss: 0.5655 - Val loss: 0.5639 - Val accuracy: 0.9921\n",
      "Epoch 32 - Train loss: 0.5672 - Val loss: 0.5710 - Val accuracy: 0.9858\n",
      "Epoch 33 - Train loss: 0.5650 - Val loss: 0.5625 - Val accuracy: 0.9890\n",
      "Epoch 34 - Train loss: 0.5627 - Val loss: 0.5632 - Val accuracy: 0.9937\n",
      "Epoch 35 - Train loss: 0.5629 - Val loss: 0.5648 - Val accuracy: 0.9858\n",
      "Epoch 36 - Train loss: 0.5608 - Val loss: 0.5617 - Val accuracy: 0.9906\n",
      "Epoch 37 - Train loss: 0.5620 - Val loss: 0.5605 - Val accuracy: 0.9890\n",
      "Epoch 38 - Train loss: 0.5605 - Val loss: 0.5599 - Val accuracy: 0.9953\n",
      "Epoch 39 - Train loss: 0.5589 - Val loss: 0.5604 - Val accuracy: 0.9937\n",
      "Epoch 40 - Train loss: 0.5591 - Val loss: 0.5592 - Val accuracy: 0.9921\n",
      "Epoch 41 - Train loss: 0.5591 - Val loss: 0.5594 - Val accuracy: 0.9937\n",
      "Epoch 42 - Train loss: 0.5594 - Val loss: 0.5609 - Val accuracy: 0.9937\n",
      "Epoch 43 - Train loss: 0.5584 - Val loss: 0.5590 - Val accuracy: 0.9921\n",
      "Epoch 44 - Train loss: 0.5573 - Val loss: 0.5632 - Val accuracy: 0.9953\n",
      "Epoch 45 - Train loss: 0.5574 - Val loss: 0.5577 - Val accuracy: 0.9969\n",
      "Epoch 46 - Train loss: 0.5583 - Val loss: 0.5622 - Val accuracy: 0.9969\n",
      "Epoch 47 - Train loss: 0.5564 - Val loss: 0.5576 - Val accuracy: 0.9969\n",
      "Epoch 48 - Train loss: 0.5564 - Val loss: 0.5627 - Val accuracy: 0.9906\n",
      "Epoch 49 - Train loss: 0.5561 - Val loss: 0.5583 - Val accuracy: 0.9969\n",
      "Epoch 50 - Train loss: 0.5553 - Val loss: 0.5566 - Val accuracy: 0.9937\n",
      "Fold 2/10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;66;03m# Train and validate for this fold\u001b[39;00m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_epochs):\n\u001b[1;32m---> 29\u001b[0m     train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     val_loss \u001b[38;5;241m=\u001b[39m train(model,val_loader,criterion,optimizer)\n\u001b[0;32m     31\u001b[0m     val_preds, val_labels \u001b[38;5;241m=\u001b[39m evaluate(model, val_loader)\n",
      "Input \u001b[1;32mIn [16]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m      3\u001b[0m model\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m      4\u001b[0m running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m train_loader:\n\u001b[0;32m      6\u001b[0m     inputs \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m      7\u001b[0m     labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:628\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    625\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    626\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    627\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 628\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    629\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    630\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    631\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[0;32m    632\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:671\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    669\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    670\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 671\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    672\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    673\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[idx] \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:58\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     56\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39m__getitems__(possibly_batched_index)\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 58\u001b[0m         data \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m possibly_batched_index]\n\u001b[0;32m     59\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torch\\utils\\data\\dataset.py:295\u001b[0m, in \u001b[0;36mSubset.__getitem__\u001b[1;34m(self, idx)\u001b[0m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(idx, \u001b[38;5;28mlist\u001b[39m):\n\u001b[0;32m    294\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindices[i] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m idx]]\n\u001b[1;32m--> 295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataset\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindices\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:229\u001b[0m, in \u001b[0;36mDatasetFolder.__getitem__\u001b[1;34m(self, index)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m    223\u001b[0m \u001b[38;5;124;03m    index (int): Index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[38;5;124;03m    tuple: (sample, target) where target is class_index of the target class.\u001b[39;00m\n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    228\u001b[0m path, target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msamples[index]\n\u001b[1;32m--> 229\u001b[0m sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    230\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    231\u001b[0m     sample \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransform(sample)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:268\u001b[0m, in \u001b[0;36mdefault_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    266\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m accimage_loader(path)\n\u001b[0;32m    267\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 268\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpil_loader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\torchvision\\datasets\\folder.py:248\u001b[0m, in \u001b[0;36mpil_loader\u001b[1;34m(path)\u001b[0m\n\u001b[0;32m    246\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m    247\u001b[0m     img \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(f)\n\u001b[1;32m--> 248\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimg\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mRGB\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\PIL\\Image.py:921\u001b[0m, in \u001b[0;36mImage.convert\u001b[1;34m(self, mode, matrix, dither, palette, colors)\u001b[0m\n\u001b[0;32m    873\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert\u001b[39m(\n\u001b[0;32m    874\u001b[0m     \u001b[38;5;28mself\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, matrix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dither\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, palette\u001b[38;5;241m=\u001b[39mPalette\u001b[38;5;241m.\u001b[39mWEB, colors\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m\n\u001b[0;32m    875\u001b[0m ):\n\u001b[0;32m    876\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;124;03m    Returns a converted copy of this image. For the \"P\" mode, this\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[38;5;124;03m    method translates pixels through the palette.  If mode is\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;124;03m    :returns: An :py:class:`~PIL.Image.Image` object.\u001b[39;00m\n\u001b[0;32m    919\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 921\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    923\u001b[0m     has_transparency \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtransparency\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m mode \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mP\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    925\u001b[0m         \u001b[38;5;66;03m# determine default mode\u001b[39;00m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\gpu2\\lib\\site-packages\\PIL\\ImageFile.py:260\u001b[0m, in \u001b[0;36mImageFile.load\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimage file is truncated \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    256\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(b)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m bytes not processed)\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    257\u001b[0m         )\n\u001b[0;32m    259\u001b[0m b \u001b[38;5;241m=\u001b[39m b \u001b[38;5;241m+\u001b[39m s\n\u001b[1;32m--> 260\u001b[0m n, err_code \u001b[38;5;241m=\u001b[39m \u001b[43mdecoder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    261\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    262\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k}')\n",
    "    \n",
    "    # Create subset datasets and dataloaders for this fold\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = SmallResNet9()\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#   optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(number_of_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer)\n",
    "        val_loss = train(model,val_loader,criterion,optimizer)\n",
    "        val_preds, val_labels = evaluate(model, val_loader)\n",
    "        val_accuracy = get_accuracy(val_preds, val_labels)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Saving the Current Fold Model\n",
    "    torch.save(model, f'fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90f0cfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# K-fold cross-validation\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(train_dataset)):\n",
    "    print(f'Fold {fold + 1}/{k}')\n",
    "    \n",
    "    # Create subset datasets and dataloaders for this fold\n",
    "    train_subset = Subset(train_dataset, train_indices)\n",
    "    val_subset = Subset(train_dataset, val_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size=64, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size=64, shuffle=False)\n",
    "    \n",
    "    # Model\n",
    "    model = torch.load(\"./model/256/3Fold-100Epochs/fold1.pth\")\n",
    "    model.to(device)\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "#   optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,weight_decay=weight_decay)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(number_of_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer)\n",
    "        val_loss = train(model,val_loader,criterion,optimizer)\n",
    "        val_preds, val_labels = evaluate(model, val_loader)\n",
    "        val_accuracy = get_accuracy(val_preds, val_labels)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Saving the Current Fold Model\n",
    "    torch.save(model, f'fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "703568e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_for_more_epochs(model,number_of_epochs):\n",
    "        # Define the loss function and optimizer\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        #   optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "        optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9,weight_decay=weight_decay)\n",
    "    \n",
    "        train_losses = []\n",
    "        val_losses = []\n",
    "        val_accuracies = []\n",
    "    \n",
    "        # Train and validate for this fold\n",
    "        for epoch in range(number_of_epochs):\n",
    "            train_loss = train(model, train_dataloader, criterion, optimizer)\n",
    "            val_loss = train(model,val_dataloader,criterion,optimizer)\n",
    "            val_preds, val_labels = evaluate(model, val_dataloader)\n",
    "            val_accuracy = get_accuracy(val_preds, val_labels)\n",
    "            train_losses.append(train_loss)\n",
    "            val_losses.append(val_loss)\n",
    "            val_accuracies.append(val_accuracy)\n",
    "            print(f'Epoch {epoch + 1} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}')\n",
    "            \n",
    "        # Saving the Current Fold Model\n",
    "        torch.save(model, f'Extra-fold{fold}.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c73203",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load(\"./model/256/3Fold-100Epochs/fold1.pth\")\n",
    "model.to(device)\n",
    "train_for_more_epochs(model,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49285d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving the model\n",
    "torch.save(model, './saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "71cfac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot the losses\n",
    "def plot_losses():\n",
    "    matplot.plot(train_losses, '-bx')\n",
    "    matplot.plot(val_losses, '-rx')\n",
    "    matplot.xlabel('epoch')\n",
    "    matplot.ylabel('loss')\n",
    "    matplot.legend(['Training', 'Validation'])\n",
    "    matplot.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4c1a0de7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7c9d1e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SmallResNet9(\n",
       "  (conv1): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "  (bn1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (layer1): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU(inplace=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (5): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "  )\n",
       "  (fc): Sequential(\n",
       "    (0): MaxPool2d(kernel_size=4, stride=4, padding=0, dilation=1, ceil_mode=False)\n",
       "    (1): Flatten(start_dim=1, end_dim=-1)\n",
       "    (2): Dropout(p=0.2, inplace=False)\n",
       "    (3): Linear(in_features=256, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing the model\n",
    "test_model = SmallResNet9()\n",
    "PATH = './fold0.pth'\n",
    "test_model = torch.load(PATH, map_location = device)\n",
    "test_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7a0a45a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Moving Test Dataloader on the device('Cuda' if available)\n",
    "test_dataloader = DeviceDataLoader(test_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d96724dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.9626, device='cuda:0')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluating Test Data\n",
    "preds, labels = evaluate(test_model, test_dataloader)\n",
    "\n",
    "# Accuracy\n",
    "accuracy = get_accuracy(preds, labels)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8482ef9d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MulticlassF1Score()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.9627)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# F-1 SCORE\n",
    "f1_score = get_f1score(preds, labels)\n",
    "f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6eac382b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "dedd7f83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: >"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cf_matrix = confusion_matrix(preds, labels)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "matplot.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c0927d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
