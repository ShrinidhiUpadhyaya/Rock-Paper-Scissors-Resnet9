{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8f7aec70",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from PIL import Image\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "from torchvision.transforms import ToTensor, Normalize, Compose\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0db4a7dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='rock-paper-scissors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e726e551",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'val']\n",
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Look into the data directory\n",
    "data_dir = './datasets/Datasets/Datasets'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bb730e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "\n",
    "train_tfms = tt.Compose([tt.CenterCrop(224),\n",
    "                         tt.Resize((32,32)), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.Resize((32,32)),tt.ToTensor(), tt.Normalize(*stats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15316eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)\n",
    "test_dl = DataLoader(test_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dcc30f0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79728bdc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c83ae26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet9(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(ResNet9, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "        )\n",
    "        \n",
    "        #         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4), \n",
    "                      nn.Flatten(), \n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, num_classes))\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1a293160",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallResNet9(nn.Module):\n",
    "    def __init__(self, num_classes=3):\n",
    "        super(SmallResNet9, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels=3, out_channels=32, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=64, out_channels=128, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=128, out_channels=128, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=2, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "#         self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "#         self.fc = nn.Linear(256, num_classes)\n",
    "        \n",
    "        self.fc = nn.Sequential(nn.MaxPool2d(4), \n",
    "                      nn.Flatten(), \n",
    "                      nn.Dropout(0.2),\n",
    "                      nn.Linear(256, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "#         x = self.avgpool(x)\n",
    "#         x = x.view(x.size(0), -1)\n",
    "        x = self.fc(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7c783c5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(SmallResNet9(),device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dba93154",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "87e71d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset and dataloader\n",
    "dataset = ImageFolder('./datasets/Datasets/Datasets/train', train_tfms)\n",
    "dataloader = DataLoader(dataset, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "# dataloader = DataLoader(dataset, batch_size=64, shuffle=True)\n",
    "dataloader = DeviceDataLoader(dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4f5d81d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 3\n",
    "number_of_epochs = 50\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1171ffeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4a37e3f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training loop function\n",
    "def train(model, train_loader, criterion, optimizer):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item() * inputs.size(0)\n",
    "    epoch_loss = running_loss / len(train_loader.dataset)\n",
    "    return epoch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "909c1e2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            all_labels.append(labels.cpu().numpy())\n",
    "            all_preds.append(preds.cpu().numpy())\n",
    "    all_labels = np.concatenate(all_labels)\n",
    "    all_preds = np.concatenate(all_preds)\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1926f8a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/3\n",
      "Epoch 1 - Train loss: 0.4687 - Val loss: 1.8222 - Val accuracy: 0.3026\n",
      "Epoch 2 - Train loss: 1.2330 - Val loss: 1.6246 - Val accuracy: 0.3403\n",
      "Epoch 3 - Train loss: 1.0973 - Val loss: 1.5592 - Val accuracy: 0.3534\n",
      "Epoch 4 - Train loss: 1.0699 - Val loss: 1.4689 - Val accuracy: 0.3702\n",
      "Epoch 5 - Train loss: 1.0090 - Val loss: 1.5601 - Val accuracy: 0.3597\n",
      "Epoch 6 - Train loss: 0.9887 - Val loss: 1.5297 - Val accuracy: 0.4084\n",
      "Epoch 7 - Train loss: 0.9282 - Val loss: 1.6198 - Val accuracy: 0.3686\n",
      "Epoch 8 - Train loss: 0.9270 - Val loss: 1.5177 - Val accuracy: 0.3571\n",
      "Epoch 9 - Train loss: 0.8214 - Val loss: 1.5337 - Val accuracy: 0.5901\n",
      "Epoch 10 - Train loss: 0.7711 - Val loss: 1.5945 - Val accuracy: 0.4660\n",
      "Epoch 11 - Train loss: 0.7656 - Val loss: 1.5151 - Val accuracy: 0.4560\n",
      "Epoch 12 - Train loss: 0.7887 - Val loss: 1.4829 - Val accuracy: 0.4272\n",
      "Epoch 13 - Train loss: 0.6959 - Val loss: 1.4454 - Val accuracy: 0.5340\n",
      "Epoch 14 - Train loss: 0.5382 - Val loss: 1.4758 - Val accuracy: 0.6157\n",
      "Epoch 15 - Train loss: 0.5727 - Val loss: 1.4236 - Val accuracy: 0.5801\n",
      "Epoch 16 - Train loss: 0.5175 - Val loss: 1.4041 - Val accuracy: 0.6613\n",
      "Epoch 17 - Train loss: 0.4844 - Val loss: 1.4117 - Val accuracy: 0.7471\n",
      "Epoch 18 - Train loss: 0.4679 - Val loss: 1.2264 - Val accuracy: 0.7178\n",
      "Epoch 19 - Train loss: 0.3251 - Val loss: 1.1554 - Val accuracy: 0.7110\n",
      "Epoch 20 - Train loss: 0.3215 - Val loss: 1.0436 - Val accuracy: 0.6105\n",
      "Epoch 21 - Train loss: 0.2781 - Val loss: 1.1049 - Val accuracy: 0.7518\n",
      "Epoch 22 - Train loss: 0.2503 - Val loss: 1.0225 - Val accuracy: 0.6424\n",
      "Epoch 23 - Train loss: 0.3108 - Val loss: 1.0966 - Val accuracy: 0.7901\n",
      "Epoch 24 - Train loss: 0.2095 - Val loss: 1.0294 - Val accuracy: 0.8639\n",
      "Epoch 25 - Train loss: 0.2244 - Val loss: 0.9919 - Val accuracy: 0.7576\n",
      "Epoch 26 - Train loss: 0.1954 - Val loss: 0.8319 - Val accuracy: 0.8110\n",
      "Epoch 27 - Train loss: 0.1621 - Val loss: 0.7247 - Val accuracy: 0.7592\n",
      "Epoch 28 - Train loss: 0.1712 - Val loss: 0.6900 - Val accuracy: 0.6058\n",
      "Epoch 29 - Train loss: 0.1594 - Val loss: 0.7572 - Val accuracy: 0.9021\n",
      "Epoch 30 - Train loss: 0.1358 - Val loss: 0.6092 - Val accuracy: 0.8686\n",
      "Epoch 31 - Train loss: 0.1131 - Val loss: 0.5289 - Val accuracy: 0.9183\n",
      "Epoch 32 - Train loss: 0.0955 - Val loss: 0.5693 - Val accuracy: 0.7560\n",
      "Epoch 33 - Train loss: 0.1269 - Val loss: 0.4701 - Val accuracy: 0.9304\n",
      "Epoch 34 - Train loss: 0.0864 - Val loss: 0.4020 - Val accuracy: 0.9230\n",
      "Epoch 35 - Train loss: 0.0771 - Val loss: 0.4733 - Val accuracy: 0.8958\n",
      "Epoch 36 - Train loss: 0.0807 - Val loss: 0.3714 - Val accuracy: 0.9340\n",
      "Epoch 37 - Train loss: 0.0715 - Val loss: 0.3895 - Val accuracy: 0.7675\n",
      "Epoch 38 - Train loss: 0.0731 - Val loss: 0.3745 - Val accuracy: 0.7827\n",
      "Epoch 39 - Train loss: 0.0640 - Val loss: 0.3298 - Val accuracy: 0.8607\n",
      "Epoch 40 - Train loss: 0.0660 - Val loss: 0.5370 - Val accuracy: 0.7901\n",
      "Epoch 41 - Train loss: 0.0869 - Val loss: 0.3850 - Val accuracy: 0.9058\n",
      "Epoch 42 - Train loss: 0.0687 - Val loss: 0.2838 - Val accuracy: 0.9361\n",
      "Epoch 43 - Train loss: 0.0569 - Val loss: 0.2250 - Val accuracy: 0.8508\n",
      "Epoch 44 - Train loss: 0.0462 - Val loss: 0.2315 - Val accuracy: 0.7901\n",
      "Epoch 45 - Train loss: 0.0500 - Val loss: 0.2156 - Val accuracy: 0.8634\n",
      "Epoch 46 - Train loss: 0.0350 - Val loss: 0.1556 - Val accuracy: 0.9524\n",
      "Epoch 47 - Train loss: 0.0254 - Val loss: 0.0916 - Val accuracy: 0.9115\n",
      "Epoch 48 - Train loss: 0.0251 - Val loss: 0.0741 - Val accuracy: 0.9565\n",
      "Epoch 49 - Train loss: 0.0223 - Val loss: 0.0370 - Val accuracy: 0.9503\n",
      "Epoch 50 - Train loss: 0.0185 - Val loss: 0.0228 - Val accuracy: 0.9539\n",
      "Fold 2/3\n",
      "Epoch 1 - Train loss: 0.4877 - Val loss: 1.8944 - Val accuracy: 0.3557\n",
      "Epoch 2 - Train loss: 1.4147 - Val loss: 1.6909 - Val accuracy: 0.2949\n",
      "Epoch 3 - Train loss: 1.2187 - Val loss: 1.5496 - Val accuracy: 0.2970\n",
      "Epoch 4 - Train loss: 1.1269 - Val loss: 1.5132 - Val accuracy: 0.3704\n",
      "Epoch 5 - Train loss: 1.0134 - Val loss: 1.6068 - Val accuracy: 0.3295\n",
      "Epoch 6 - Train loss: 1.2067 - Val loss: 1.4202 - Val accuracy: 0.3960\n",
      "Epoch 7 - Train loss: 1.0425 - Val loss: 1.5331 - Val accuracy: 0.3478\n",
      "Epoch 8 - Train loss: 1.0617 - Val loss: 1.4195 - Val accuracy: 0.4301\n",
      "Epoch 9 - Train loss: 0.9645 - Val loss: 1.5074 - Val accuracy: 0.3892\n",
      "Epoch 10 - Train loss: 0.9322 - Val loss: 1.5430 - Val accuracy: 0.4620\n",
      "Epoch 11 - Train loss: 0.8966 - Val loss: 1.5312 - Val accuracy: 0.3546\n",
      "Epoch 12 - Train loss: 0.8732 - Val loss: 1.5400 - Val accuracy: 0.5060\n",
      "Epoch 13 - Train loss: 0.8142 - Val loss: 1.6857 - Val accuracy: 0.3169\n",
      "Epoch 14 - Train loss: 1.0467 - Val loss: 1.3176 - Val accuracy: 0.5479\n",
      "Epoch 15 - Train loss: 0.8152 - Val loss: 1.5089 - Val accuracy: 0.4814\n",
      "Epoch 16 - Train loss: 1.0144 - Val loss: 1.3803 - Val accuracy: 0.3546\n",
      "Epoch 17 - Train loss: 0.9284 - Val loss: 1.4139 - Val accuracy: 0.5600\n",
      "Epoch 18 - Train loss: 0.7086 - Val loss: 1.4765 - Val accuracy: 0.4751\n",
      "Epoch 19 - Train loss: 0.8593 - Val loss: 1.5009 - Val accuracy: 0.5794\n",
      "Epoch 20 - Train loss: 0.6989 - Val loss: 1.4480 - Val accuracy: 0.6710\n",
      "Epoch 21 - Train loss: 0.5479 - Val loss: 1.4250 - Val accuracy: 0.5547\n",
      "Epoch 22 - Train loss: 0.5720 - Val loss: 1.3692 - Val accuracy: 0.6228\n",
      "Epoch 23 - Train loss: 0.4217 - Val loss: 1.3365 - Val accuracy: 0.6867\n",
      "Epoch 24 - Train loss: 0.4685 - Val loss: 1.2283 - Val accuracy: 0.7889\n",
      "Epoch 25 - Train loss: 0.3607 - Val loss: 1.2327 - Val accuracy: 0.5380\n",
      "Epoch 26 - Train loss: 0.5180 - Val loss: 1.1410 - Val accuracy: 0.7795\n",
      "Epoch 27 - Train loss: 0.3286 - Val loss: 1.0280 - Val accuracy: 0.6579\n",
      "Epoch 28 - Train loss: 0.4354 - Val loss: 0.9597 - Val accuracy: 0.7606\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [17]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;66;03m# Train and validate for this fold\u001b[39;00m\n\u001b[0;32m     30\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(number_of_epochs):\n\u001b[1;32m---> 31\u001b[0m         train_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m#         val_loss = evaluate(model, val_loader)\u001b[39;00m\n\u001b[0;32m     33\u001b[0m         val_loss \u001b[38;5;241m=\u001b[39m train(model,val_loader,criterion,optimizer)\n",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, train_loader, criterion, optimizer)\u001b[0m\n\u001b[0;32m     11\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n\u001b[0;32m     12\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m---> 13\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m inputs\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     14\u001b[0m epoch_loss \u001b[38;5;241m=\u001b[39m running_loss \u001b[38;5;241m/\u001b[39m \u001b[38;5;28mlen\u001b[39m(train_loader\u001b[38;5;241m.\u001b[39mdataset)\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m epoch_loss\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Define k-fold cross-validation\n",
    "\n",
    "kf = KFold(n_splits=k, shuffle=True)\n",
    "\n",
    "# Initialize lists to store training and validation losses and accuracies\n",
    "\n",
    "\n",
    "# Train and validate for each fold\n",
    "for fold, (train_indices, val_indices) in enumerate(kf.split(dataset)):\n",
    "    print(f'Fold {fold + 1}/{k}')\n",
    "    \n",
    "    # Create subset datasets and dataloaders for this fold\n",
    "    train_subset = Subset(dataset, train_indices)\n",
    "    val_subset = Subset(dataset, val_indices)\n",
    "    train_loader = DataLoader(train_subset, batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(val_subset, batch_size, shuffle=False)\n",
    "    \n",
    "    # Define model,\n",
    "    model = SmallResNet9()\n",
    "    model.to(device)\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), learning_rate)\n",
    "    \n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    # Train and validate for this fold\n",
    "    for epoch in range(number_of_epochs):\n",
    "        train_loss = train(model, train_loader, criterion, optimizer)\n",
    "#         val_loss = evaluate(model, val_loader)\n",
    "        val_loss = train(model,val_loader,criterion,optimizer)\n",
    "        val_accuracy = evaluate(model, val_loader)\n",
    "        train_losses.append(train_loss)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_accuracy)\n",
    "        print(f'Epoch {epoch + 1} - Train loss: {train_loss:.4f} - Val loss: {val_loss:.4f} - Val accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    # Save the model for this fold\n",
    "    torch.save(model.state_dict(), f'resnet9_fold{fold}.pt')\n",
    "    \n",
    "# Print overall validation accuracy\n",
    "print(f'Overall validation accuracy: {np.mean(val_accuracies):.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49285d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71cfac3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses():\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c1a0de7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7c9d1e5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
