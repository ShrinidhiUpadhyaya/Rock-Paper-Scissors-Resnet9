{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82601c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torchvision\n",
    "import tarfile\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as tt\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# matplotlib.rcParams['figure.facecolor'] = '#ffffff'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "343d54f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name='rock-paper-scissors'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1e1da45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['test', 'train', 'validation']\n",
      "['paper', 'rock', 'scissors']\n"
     ]
    }
   ],
   "source": [
    "    \n",
    "# Look into the data directory\n",
    "data_dir = './datasets/complex-images'\n",
    "print(os.listdir(data_dir))\n",
    "classes = os.listdir(data_dir + \"/train\")\n",
    "print(classes)\n",
    "\n",
    "# data_dir = \"./archive\"\n",
    "# print(os.listdir(data_dir))\n",
    "# classes = os.listdir(data_dir + \"/Rock-Paper-Scissors/train\")\n",
    "# print(classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51fd184",
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "\n",
    "\n",
    "# load the image\n",
    "img_path = data_dir + \"/train/paper/glu_174.png\"\n",
    "img = Image.open(img_path)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "]) \n",
    "# transform the pIL image to tensor\n",
    "# image\n",
    "img_tr = transform(img)\n",
    "\n",
    "img_tr = transform(img)\n",
    " \n",
    "# calculate mean and std\n",
    "mean, std = img_tr.mean([1,2]), img_tr.std([1,2])\n",
    " \n",
    "# print mean and std\n",
    "print(\"mean and std before normalize:\")\n",
    "print(\"Mean of the image:\", mean)\n",
    "print(\"Std of the image:\", std)\n",
    "\n",
    "transform_norm = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std)\n",
    "])\n",
    " \n",
    "# get normalized image\n",
    "img_normalized = transform_norm(img)\n",
    "\n",
    "# Data transforms (normalization & data augmentation)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80254275",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Working for common images\n",
    "# stats = ((0.4914, 0.4822, 0.4465), (0.2023, 0.1994, 0.2010))\n",
    "#Working for normal white images\n",
    "#stats = ((0.8763, 0.8430, 0.8365), (0.2030, 0.2536, 0.2634))\n",
    "\n",
    "#For complex with face images\n",
    "# stats = ((0.3754, 0.3303, 0.3217), (0.2652, 0.2442, 0.2305))\n",
    "\n",
    "stats=((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
    "# stats=((-0.8244, -1.0126, -0.7527)), ((1.2136, 1.1163, 1.0315))\n",
    "\n",
    "\n",
    "train_tfms = tt.Compose([tt.Resize((32,32)), \n",
    "                         tt.RandomHorizontalFlip(), \n",
    "                         tt.ToTensor(), \n",
    "                         tt.Normalize(*stats,inplace=True)])\n",
    "valid_tfms = tt.Compose([tt.Resize((32,32)),tt.ToTensor(), tt.Normalize(*stats)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3cddc26b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torchvision.datasets.folder.ImageFolder'>\n"
     ]
    }
   ],
   "source": [
    "# PyTorch datasets\n",
    "train_ds = ImageFolder(data_dir+'/train', train_tfms)\n",
    "valid_ds = ImageFolder(data_dir+'/test', valid_tfms)\n",
    "test_ds = ImageFolder(data_dir+'/validation', valid_tfms)\n",
    "\n",
    "print(type(valid_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f9c5c64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "50d5f143",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch data loaders\n",
    "train_dl = DataLoader(train_ds, batch_size, shuffle=True, num_workers=3, pin_memory=True)\n",
    "valid_dl = DataLoader(valid_ds, batch_size*2, num_workers=3, pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc7ea5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_batch(dl):\n",
    "    for images, labels in dl:\n",
    "        fig, ax = plt.subplots(figsize=(12, 12))\n",
    "        ax.set_xticks([]); ax.set_yticks([])\n",
    "        ax.imshow(make_grid(images[:64], nrow=8).permute(1, 2, 0))\n",
    "        break\n",
    "show_batch(train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "64e42987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_device():\n",
    "    \"\"\"Pick GPU if available, else CPU\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        return torch.device('cuda')\n",
    "    else:\n",
    "        return torch.device('cpu')\n",
    "    \n",
    "def to_device(data, device):\n",
    "    \"\"\"Move tensor(s) to chosen device\"\"\"\n",
    "    if isinstance(data, (list,tuple)):\n",
    "        return [to_device(x, device) for x in data]\n",
    "    return data.to(device, non_blocking=True)\n",
    "\n",
    "class DeviceDataLoader():\n",
    "    \"\"\"Wrap a dataloader to move data to a device\"\"\"\n",
    "    def __init__(self, dl, device):\n",
    "        self.dl = dl\n",
    "        self.device = device\n",
    "        \n",
    "    def __iter__(self):\n",
    "        \"\"\"Yield a batch of data after moving it to device\"\"\"\n",
    "        for b in self.dl: \n",
    "            yield to_device(b, self.device)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Number of batches\"\"\"\n",
    "        return len(self.dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6c6d0d55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = get_default_device()\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "718d4af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dl = DeviceDataLoader(train_dl, device)\n",
    "valid_dl = DeviceDataLoader(valid_dl, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "239e9b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(outputs, labels):\n",
    "    _, preds = torch.max(outputs, dim=1)\n",
    "    return torch.tensor(torch.sum(preds == labels).item() / len(preds))\n",
    "\n",
    "class ImageClassificationBase(nn.Module):\n",
    "    def training_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                  # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels) # Calculate loss\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch):\n",
    "        images, labels = batch \n",
    "        out = self(images)                    # Generate predictions\n",
    "        loss = F.cross_entropy(out, labels)   # Calculate loss\n",
    "        acc = accuracy(out, labels)           # Calculate accuracy\n",
    "        return {'val_loss': loss.detach(), 'val_acc': acc}\n",
    "        \n",
    "    def validation_epoch_end(self, outputs):\n",
    "        batch_losses = [x['val_loss'] for x in outputs]\n",
    "        epoch_loss = torch.stack(batch_losses).mean()   # Combine losses\n",
    "        batch_accs = [x['val_acc'] for x in outputs]\n",
    "        epoch_acc = torch.stack(batch_accs).mean()      # Combine accuracies\n",
    "        return {'val_loss': epoch_loss.item(), 'val_acc': epoch_acc.item()}\n",
    "    \n",
    "    def epoch_end(self, epoch, result):\n",
    "        print(\"Epoch [{}], last_lr: {:.5f}, train_loss: {:.4f}, val_loss: {:.4f}, val_acc: {:.4f}\".format(\n",
    "            epoch, result['lrs'][-1], result['train_loss'], result['val_loss'], result['val_acc']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f6f7714b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def conv_block(in_channels, out_channels, pool=False):\n",
    "    layers = [nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1), \n",
    "              nn.BatchNorm2d(out_channels), \n",
    "              nn.ReLU(inplace=True)]\n",
    "    if pool: layers.append(nn.MaxPool2d(2))\n",
    "    return nn.Sequential(*layers)\n",
    "\n",
    "class ResNet9(ImageClassificationBase):\n",
    "    def __init__(self, in_channels, num_classes):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.conv1 = conv_block(in_channels, 64)\n",
    "        self.conv2 = conv_block(64, 128, pool=True)\n",
    "        self.res1 = nn.Sequential(conv_block(128, 128), conv_block(128, 128))\n",
    "        \n",
    "        self.conv3 = conv_block(128, 256, pool=True)\n",
    "        self.conv4 = conv_block(256, 512, pool=True)\n",
    "        self.res2 = nn.Sequential(conv_block(512, 512), conv_block(512, 512))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.MaxPool2d(4), \n",
    "                                        nn.Flatten(), \n",
    "                                        nn.Dropout(0.2),\n",
    "                                        nn.Linear(512, num_classes))\n",
    "        \n",
    "    def forward(self, xb):\n",
    "        out = self.conv1(xb)\n",
    "        out = self.conv2(out)\n",
    "        out = self.res1(out)\n",
    "        out = self.conv3(out)\n",
    "        out = self.conv4(out)\n",
    "        out = self.res2(out)\n",
    "        out = self.classifier(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7ccbf388",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = to_device(ResNet9(3, 3), device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b646a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchsummary import summary\n",
    "summary(model,(3,32,32),128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89cdcdc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "\n",
    "k=3\n",
    "splits=KFold(n_splits=k,shuffle=True,random_state=42)\n",
    "dataset = ConcatDataset([train_ds, valid_ds])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54ae5ea6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# @torch.no_grad()\n",
    "# def evaluate(model, val_loader):\n",
    "#     model.eval()\n",
    "#     outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "#     return model.validation_epoch_end(outputs)\n",
    "\n",
    "# def get_lr(optimizer):\n",
    "#     for param_group in optimizer.param_groups:\n",
    "#         return param_group['lr']\n",
    "\n",
    "# def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "#                   weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "#     torch.cuda.empty_cache()\n",
    "#     history = []\n",
    "    \n",
    "#     # Set up cutom optimizer with weight decay\n",
    "#     optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "#     # Set up one-cycle learning rate scheduler\n",
    "#     sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "#                                                 steps_per_epoch=len(train_loader))\n",
    "#     dataset = ConcatDataset([train_ds, valid_ds])\n",
    "#     for fold, (train_idx,val_idx) in enumerate(splits.split(np.arange(len(dataset)))):\n",
    "#         print(\"Fold:-\", fold)\n",
    "        \n",
    "#         for epoch in range(epochs):\n",
    "#             # Training Phase \n",
    "#             model.train()\n",
    "#             train_losses = []\n",
    "#             lrs = []\n",
    "#             for batch in train_loader:\n",
    "#                 loss = model.training_step(batch)\n",
    "#                 train_losses.append(loss)\n",
    "#                 loss.backward()\n",
    "\n",
    "#                 # Gradient clipping\n",
    "#                 if grad_clip: \n",
    "#                     nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "\n",
    "#                 optimizer.step()\n",
    "#                 optimizer.zero_grad()\n",
    "                \n",
    "#                 # Record & update learning rate\n",
    "#                 lrs.append(get_lr(optimizer))\n",
    "                                \n",
    "# #                 sched.step()\n",
    "                \n",
    "\n",
    "#             # Validation phase\n",
    "#             result = evaluate(model, val_loader)\n",
    "#             result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "#             result['lrs'] = lrs\n",
    "#             model.epoch_end(epoch, result)\n",
    "#             history.append(result)\n",
    "#     return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4c06df0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def evaluate(model, val_loader):\n",
    "    model.eval()\n",
    "    outputs = [model.validation_step(batch) for batch in val_loader]\n",
    "    return model.validation_epoch_end(outputs)\n",
    "\n",
    "def get_lr(optimizer):\n",
    "    for param_group in optimizer.param_groups:\n",
    "        return param_group['lr']\n",
    "\n",
    "def fit_one_cycle(epochs, max_lr, model, train_loader, val_loader, \n",
    "                  weight_decay=0, grad_clip=None, opt_func=torch.optim.SGD):\n",
    "    torch.cuda.empty_cache()\n",
    "    history = []\n",
    "    \n",
    "    # Set up cutom optimizer with weight decay\n",
    "    optimizer = opt_func(model.parameters(), max_lr, weight_decay=weight_decay)\n",
    "    # Set up one-cycle learning rate scheduler\n",
    "    sched = torch.optim.lr_scheduler.OneCycleLR(optimizer, max_lr, epochs=epochs, \n",
    "                                                steps_per_epoch=len(train_loader))\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Training Phase \n",
    "        model.train()\n",
    "        train_losses = []\n",
    "        lrs = []\n",
    "        for batch in train_loader:\n",
    "            loss = model.training_step(batch)\n",
    "            train_losses.append(loss)\n",
    "            loss.backward()\n",
    "            \n",
    "            # Gradient clipping\n",
    "            if grad_clip: \n",
    "                nn.utils.clip_grad_value_(model.parameters(), grad_clip)\n",
    "            \n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Record & update learning rate\n",
    "            lrs.append(get_lr(optimizer))\n",
    "            sched.step()\n",
    "        \n",
    "        # Validation phase\n",
    "        result = evaluate(model, val_loader)\n",
    "        result['train_loss'] = torch.stack(train_losses).mean().item()\n",
    "        result['lrs'] = lrs\n",
    "        model.epoch_end(epoch, result)\n",
    "        history.append(result)\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2666f6a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'val_loss': 1.101110577583313, 'val_acc': 0.29296875}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history = [evaluate(model, valid_dl)]\n",
    "history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d9c3cd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 8\n",
    "max_lr = 0.01\n",
    "grad_clip = 0.1\n",
    "weight_decay = 1e-4\n",
    "opt_func = torch.optim.Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "528f74ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], last_lr: 0.00372, train_loss: 1.0064, val_loss: 2.6151, val_acc: 0.3242\n",
      "Epoch [1], last_lr: 0.00932, train_loss: 0.5325, val_loss: 8.8367, val_acc: 0.5599\n",
      "Epoch [2], last_lr: 0.00972, train_loss: 0.4525, val_loss: 2.8280, val_acc: 0.5193\n",
      "Epoch [3], last_lr: 0.00812, train_loss: 0.4439, val_loss: 1.4506, val_acc: 0.5720\n",
      "Epoch [4], last_lr: 0.00556, train_loss: 0.3282, val_loss: 2.2271, val_acc: 0.5538\n",
      "Epoch [5], last_lr: 0.00283, train_loss: 0.2482, val_loss: 1.0279, val_acc: 0.6324\n",
      "Epoch [6], last_lr: 0.00077, train_loss: 0.1933, val_loss: 1.1456, val_acc: 0.6373\n",
      "Epoch [7], last_lr: 0.00000, train_loss: 0.1702, val_loss: 1.2015, val_acc: 0.6390\n",
      "Wall time: 3min 35s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "history += fit_one_cycle(epochs, max_lr, model, train_dl, valid_dl, \n",
    "                             grad_clip=grad_clip, \n",
    "                             weight_decay=weight_decay, \n",
    "                             opt_func=opt_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7caabf4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, './saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b10861",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.load('./saved_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1af94264",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_losses(history):\n",
    "    train_losses = [x.get('train_loss') for x in history]\n",
    "    val_losses = [x['val_loss'] for x in history]\n",
    "    plt.plot(train_losses, '-bx')\n",
    "    plt.plot(val_losses, '-rx')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.ylabel('loss')\n",
    "    plt.legend(['Training', 'Validation'])\n",
    "    plt.title('Loss vs. No. of epochs');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697a952c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_losses(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4730d0c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_lrs(history):\n",
    "    lrs = np.concatenate([x.get('lrs', []) for x in history])\n",
    "    plt.plot(lrs)\n",
    "    plt.xlabel('Batch no.')\n",
    "    plt.ylabel('Learning rate')\n",
    "    plt.title('Learning Rate vs. Batch no.');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9943c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_lrs(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfa47187",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(img, model):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "    # Retrieve the class label\n",
    "    return train_ds.classes[preds[0].item()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f62149",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "img, label = valid_ds[450]\n",
    "\n",
    "mean, std = img.mean([1,2]), img.std([1,2])\n",
    "\n",
    "print(mean,std)\n",
    "plt.imshow(img.permute(1, 2, 0).clamp(0, 1))\n",
    "print('Label:', valid_ds.classes[label], ', Predicted:', predict_image(img, model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73f0e540",
   "metadata": {},
   "source": [
    "##### from PIL import Image\n",
    "\n",
    "# Read a PIL image\n",
    "image = Image.open('./archive2/test/paper/nasmi_166.png')\n",
    "plt.imshow(image)\n",
    "plt.show()  \n",
    "\n",
    "print(image.size)\n",
    "\n",
    "stats_test = ((0.8763, 0.8430, 0.8365), (0.2030, 0.2536, 0.2634))\n",
    "test_tfms = tt.Compose([tt.Resize((32,32)),tt.ToTensor(), tt.Normalize(*stats_test)])\n",
    "test_ds = ImageFolder('./archive2/test', test_tfms)\n",
    "test_dl = DataLoader(test_ds, batch_size*2, num_workers=3, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "51ce4792",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sn\n",
    "import pandas as pd\n",
    "\n",
    "y_pred = []\n",
    "y_true = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "01d6bfdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image_for_accuracy(img):\n",
    "    # Convert to a batch of 1\n",
    "    xb = to_device(img.unsqueeze(0), device)\n",
    "    # Get predictions from model\n",
    "    yb = model(xb)\n",
    "    # Pick index with highest probability\n",
    "    _, preds  = torch.max(yb, dim=1)\n",
    "#     print(preds[0])\n",
    "    # Retrieve the class label\n",
    "    return preds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "9fb72ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66\n"
     ]
    }
   ],
   "source": [
    "print(len(test_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "c03675ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 0, 2, 2, 2, 2, 1, 2, 2, 1, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 1, 0, 1, 2, 1, 1, 1, 1, 0, 0, 1, 2, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 1, 0, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x504 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "predicted_image_list = []\n",
    "\n",
    "# image, label = valid_ds[10]\n",
    "\n",
    "# print(type(image))\n",
    "# a = predict_image_for_accuracy(image)\n",
    "# print(a)\n",
    "# print(type(a))a\n",
    "# print(type(label))\n",
    "# print(valid_ds.classes[a])\n",
    "# print(valid_ds.classes[label])\n",
    "\n",
    "for i in range(66):\n",
    "    image, label = test_ds[i]\n",
    "    \n",
    "    output = predict_image_for_accuracy(image) # Feed Network\n",
    "    y_pred.append(output.item()) # Save Prediction    \n",
    "    y_true.append(label) # Save Truth \n",
    "\n",
    "print(y_pred)\n",
    "print(y_true)\n",
    "# constant for classes\n",
    "classes = ('Rock', 'Paper', 'Scissors')\n",
    "\n",
    "# Build confusion matrix\n",
    "cf_matrix = confusion_matrix(y_true, y_pred)\n",
    "df_cm = pd.DataFrame(cf_matrix / np.sum(cf_matrix, axis=1), index = [i for i in classes],\n",
    "                     columns = [i for i in classes])\n",
    "plt.figure(figsize = (12,7))\n",
    "sn.heatmap(df_cm, annot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc6455b1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ed7111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_class_accuracy(outputs, targets, class_idx):\n",
    "    \"\"\"Calculates the accuracy for a specific class\n",
    "    Args:\n",
    "        outputs (torch.Tensor): The model outputs (logits) of shape (batch_size, num_classes)\n",
    "        targets (torch.Tensor): The true labels of shape (batch_size,)\n",
    "        class_idx (int): The index of the class for which accuracy is to be calculated\n",
    "    Returns:\n",
    "        float: The accuracy for the specific class\n",
    "    \"\"\"\n",
    "    class_correct = (outputs.argmax(dim=1) == targets).sum().float()\n",
    "    class_total = (targets == class_idx).sum().float()\n",
    "\n",
    "    return class_correct / class_total if class_total > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148153f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate_class_accuracy(,valid_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a260455",
   "metadata": {},
   "outputs": [],
   "source": [
    "paper_images = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1e84ba4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
